{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Lemmatization/Stemming in python\n",
    " \n",
    "- The goal of this notebok is to demonstrate the word stemming capabilities of the nltk and spaCy package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import porter, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/nmiles/PACMan_dist/libs/stopwords.txt', 'r') as test_file:\n",
    "    text = test_file.readlines()\n",
    "    stop_words = [val.strip('\\n') for val in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some example text to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./0896.pdf.txtx', 'r') as test_file:\n",
    "    text = test_file.readlines()\n",
    "    text = [val.strip('\\n') for val in text]\n",
    "# text = ' '.join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [val.strip('\\n') for val in text]\n",
    "# text = ' '.join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = [val.split(' ')[0] for val in text if val != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in lexicon[:10]:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = [word_tokenize(word) for word in lexicon if len(word) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk2wn_tag(nltk_tag):\n",
    "    \"\"\"Convenience function for converting NLTK POS tags to wordnet equivalents\n",
    "    \"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # If it's unclear what it is, just assume the default [NOUN]\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the proper parts-of-speech tag for each token and convert them from NLTK to wordnet\n",
    "final_lexicon = []\n",
    "for lex in lexicon:\n",
    "    pos_tag = nltk.pos_tag(lex)\n",
    "    wdnet_pos_tag = nltk2wn_tag(pos_tag[0][1])\n",
    "    final_lexicon.append((lex[0], wdnet_pos_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lexicon[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wordnet lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the robust Porter Stemmer\n",
    "stemmer = porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lex in final_lexicon[:10]:\n",
    "    print(f'Word: {lex} \\nLemma: {lemmatizer.lemmatize(lex[0], pos=lex[1])}\\nStem: {stemmer.stem(lex[0])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same steps using a class-based approach with spaCy.\n",
    "\n",
    "- spaCy is different in that it prefers to receive the abstract in a single chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stop = set(nlp.Defaults.stop_words)\n",
    "custom_stop = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stop_words = custom_stop.difference(spacy_stop)\n",
    "print(len(missing_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine them into a single list of stop words\n",
    "nlp.Defaults.stop_words |= set(missing_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./0896.pdf.txtx', 'r') as test_file:\n",
    "    text = test_file.readlines()\n",
    "    text = [val.strip('\\n') for val in text]\n",
    "    text = ' '.join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = nlp(text)\n",
    "print(len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(abstract))\n",
    "print(abstract[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_stop_words = []\n",
    "autogen_stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in abstract:\n",
    "    if token.is_stop:\n",
    "        autogen_stop_words.append(token)\n",
    "        continue\n",
    "    trim_stop_words.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trim_stop_words)/len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(text, return_type='str'):\n",
    "    # Creating our token object, which contains each word token parsed from the text.\n",
    "    mytokens = parser(text)\n",
    "    num_tokens = len(mytokens)\n",
    "    # Next, lemmatize each token and standardize the capitalization to be lower case\n",
    "    mytokens = [\n",
    "        word.lemma_.lower().strip()\n",
    "        if word.lemma_ != \"-PRON-\" else word.lower_ \n",
    "        for word in mytokens \n",
    "    ]\n",
    "\n",
    "    # Removing stop words and punctuation\n",
    "    mytokens = [\n",
    "        word for word in mytokens \n",
    "        if word not in stop_words and word not in punctuations\n",
    "    ]\n",
    "    print(f\"Processed text represents {len(mytokens)/num_tokens:0.2f}% of the input text\")\n",
    "    \n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = spacy_tokenizer(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
